{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "rounds = 2\n",
    "local_epoch = 2\n",
    "users = 1 # number of clients\n",
    "port = 1969#10080"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import seaborn as sns\n",
    "import random\n",
    "import cv2\n",
    "import copy\n",
    "import os\n",
    "import pdb\n",
    "import time\n",
    "import gc\n",
    "from scipy.io import loadmat\n",
    "\n",
    "import PIL\n",
    "import PIL.ImageOps\n",
    "import PIL.ImageEnhance\n",
    "import PIL.ImageDraw\n",
    "from PIL import Image\n",
    "\n",
    "from collections import namedtuple, defaultdict\n",
    "from torch.jit.annotations import Optional\n",
    "from copy import copy\n",
    "from itertools import cycle\n",
    "\n",
    "import torch\n",
    "from torch import nn,optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "import copy\n",
    "\n",
    "# will clean later\n",
    "\n",
    "import os\n",
    "import h5py\n",
    "\n",
    "import socket\n",
    "import struct\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from threading import Thread\n",
    "from threading import Lock\n",
    "\n",
    "\n",
    "import time\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_msg(sock, msg):\n",
    "    # prefix each message with a 4-byte length in network byte order\n",
    "    msg = pickle.dumps(msg)\n",
    "    l_send = len(msg)\n",
    "    msg = struct.pack('>I', l_send) + msg\n",
    "    sock.sendall(msg)\n",
    "    return l_send\n",
    "\n",
    "def recv_msg(sock):\n",
    "    # read message length and unpack it into an integer\n",
    "    raw_msglen = recvall(sock, 4)\n",
    "    if not raw_msglen:\n",
    "        return None\n",
    "    msglen = struct.unpack('>I', raw_msglen)[0]\n",
    "    # read the message data\n",
    "    msg =  recvall(sock, msglen)\n",
    "    msg = pickle.loads(msg)\n",
    "    return msg, msglen\n",
    "\n",
    "def recvall(sock, n):\n",
    "    # helper function to receive n bytes or return None if EOF is hit\n",
    "    data = b''\n",
    "    while len(data) < n:\n",
    "        packet = sock.recv(n - len(data))\n",
    "        if not packet:\n",
    "            return None\n",
    "        data += packet\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_weights(w, datasize):\n",
    "    \"\"\"\n",
    "    Returns the average of the weights.\n",
    "    \"\"\"\n",
    "        \n",
    "    for i, data in enumerate(datasize):\n",
    "        for key in w[i].keys():\n",
    "            w[i][key] *= float(data)\n",
    "    \n",
    "    w_avg = copy.deepcopy(w[0])\n",
    "    \n",
    "    \n",
    "\n",
    "# when client use only one kinds of device\n",
    "\n",
    "    for key in w_avg.keys():\n",
    "        for i in range(1, len(w)):\n",
    "            w_avg[key] += w[i][key]\n",
    "        w_avg[key] = torch.div(w_avg[key], float(sum(datasize)))\n",
    "\n",
    "# when client use various devices (cpu, gpu) you need to use it instead\n",
    "#\n",
    "#     for key, val in w_avg.items():\n",
    "#         common_device = val.device\n",
    "#         break\n",
    "#     for key in w_avg.keys():\n",
    "#         for i in range(1, len(w)):\n",
    "#             if common_device == 'cpu':\n",
    "#                 w_avg[key] += w[i][key].cpu()\n",
    "#             else:\n",
    "#                 w_avg[key] += w[i][key].cuda()\n",
    "#         w_avg[key] = torch.div(w_avg[key], float(sum(datasize)))\n",
    "\n",
    "    return w_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_thread(func, num_user):\n",
    "    global clientsoclist\n",
    "    global start_time\n",
    "    \n",
    "    thrs = []\n",
    "    for i in range(num_user):\n",
    "        conn, addr = s.accept()\n",
    "        print('Conntected with', addr)\n",
    "        # append client socket on list\n",
    "        clientsoclist[i] = conn\n",
    "        args = (i, num_user, conn)\n",
    "        thread = Thread(target=func, args=args)\n",
    "        thrs.append(thread)\n",
    "        thread.start()\n",
    "    print(\"timer starts!\")\n",
    "    start_time = time.time()    # store start time\n",
    "    for thread in thrs:\n",
    "        thread.join()\n",
    "    end_time = time.time()  # store end time\n",
    "    print(\"TrainingTime: {} sec\".format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def receive(userid, num_users, conn): #thread for receive clients\n",
    "    global weight_count\n",
    "    \n",
    "    global datasetsize\n",
    "\n",
    "    #change here\n",
    "    msg = {\n",
    "        'rounds': rounds,\n",
    "        'client_id': userid,\n",
    "        'local_epoch': local_epoch\n",
    "    }\n",
    "\n",
    "    datasize = send_msg(conn, msg)    #send epoch\n",
    "\n",
    "    \n",
    "    total_sendsize_list.append(datasize)\n",
    "    client_sendsize_list[userid].append(datasize)\n",
    "\n",
    "    train_dataset_size, datasize = recv_msg(conn)    # get total_batch of train dataset\n",
    " \n",
    "    \n",
    "    total_receivesize_list.append(datasize)\n",
    "    client_receivesize_list[userid].append(datasize)\n",
    "    \n",
    "    \n",
    "    with lock:\n",
    "        datasetsize[userid] = train_dataset_size\n",
    "        weight_count += 1\n",
    "    \n",
    "    train(userid, train_dataset_size, num_users, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(userid, train_dataset_size, num_users, client_conn):\n",
    "    global weights_list\n",
    "    global global_weights\n",
    "    global weight_count\n",
    "    global ecg_net\n",
    "    global val_acc\n",
    "    \n",
    "    for r in range(rounds):\n",
    "        with lock:\n",
    "            if weight_count == num_users:\n",
    "                for i, conn in enumerate(clientsoclist):\n",
    "                    datasize = send_msg(conn, global_weights)\n",
    "                    total_sendsize_list.append(datasize)\n",
    "                    client_sendsize_list[i].append(datasize)\n",
    "                    train_sendsize_list.append(datasize)\n",
    "                    weight_count = 0\n",
    "\n",
    "        client_weights, datasize = recv_msg(client_conn)\n",
    "        total_receivesize_list.append(datasize)\n",
    "        client_receivesize_list[userid].append(datasize)\n",
    "        train_receivesize_list.append(datasize)\n",
    "\n",
    "        weights_list[userid] = client_weights\n",
    "        print(\"User\" + str(userid) + \"'s Round \" + str(r + 1) +  \" is done\")\n",
    "        with lock:\n",
    "            weight_count += 1\n",
    "            if weight_count == num_users:\n",
    "                #average\n",
    "                global_weights = average_weights(weights_list, datasetsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E7XRZIKtCN8E"
   },
   "source": [
    "# Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "id": "-6d0QadwGDl_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "def double_conv(in_channels, out_channels):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
    "        nn.InstanceNorm2d(out_channels),\n",
    "        nn.LeakyReLU(0.2, inplace=True),\n",
    "        nn.Conv2d(out_channels, out_channels, 3, padding=1),\n",
    "        nn.InstanceNorm2d(in_channels),\n",
    "        nn.LeakyReLU(0.2, inplace=True),\n",
    "        nn.Dropout(0.25)\n",
    "    )   \n",
    "\n",
    "#'''\n",
    "def generate_saliency(inputs, encoder, optimizer):\n",
    "  inputs2 = copy(inputs)\n",
    "  inputs2.requires_grad = True\n",
    "  encoder.eval()\n",
    "\n",
    "  conv5, conv4, conv3, conv2, conv1, scores = encoder(inputs2)\n",
    "\n",
    "  score_max, score_max_index = torch.max(scores, 1)\n",
    "  score_max.backward(torch.FloatTensor([1.0]*score_max.shape[0]).to(device))\n",
    "  saliency, _ = torch.max(inputs2.grad.data.abs(),dim=1)\n",
    "  saliency = inputs2.grad.data.abs()\n",
    "  optimizer.zero_grad()\n",
    "  encoder.train()\n",
    "\n",
    "  return saliency\n",
    "#'''\n",
    "\n",
    "'''\n",
    "def generate_saliency(inputs, encoder, optimizer):\n",
    "  #inputs2 = copy(inputs)\n",
    "  inputs.requires_grad = True\n",
    "  encoder.eval()\n",
    "\n",
    "  conv5, conv4, conv3, conv2, conv1, scores = encoder(inputs)\n",
    "\n",
    "  score_max, score_max_index = torch.max(scores, 1)\n",
    "  score_max.backward(torch.FloatTensor([1.0]*score_max.shape[0]).to(device))\n",
    "  saliency, _ = torch.max(inputs.grad.data.abs(),dim=1)\n",
    "  saliency = inputs.grad.data.abs()\n",
    "  optimizer.zero_grad()\n",
    "  encoder.train()\n",
    "\n",
    "  return saliency\n",
    "  \n",
    "'''\n",
    "\n",
    "class CovidMix(nn.Module):\n",
    "\n",
    "    def __init__(self, n_class = 1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = Encoder(1)\n",
    "        self.decoder = Decoder(1)\n",
    "        self.generate_saliency = generate_saliency\n",
    "        \n",
    "\n",
    "    def forward(self, x, optimizer):\n",
    "        \n",
    "        saliency = self.generate_saliency(x, self.encoder, optimizer)\n",
    "        conv5, conv4, conv3, conv2, conv1, outC = self.encoder(x)\n",
    "        outSeg = self.decoder(x, conv5, conv4, conv3, conv2, conv1, saliency)\n",
    "\n",
    "        # return outSeg, outC, saliency\n",
    "        return outSeg, outC\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, n_class = 1):\n",
    "        super().__init__()\n",
    "                \n",
    "        self.dconv_down1 = double_conv(1, 16)\n",
    "        self.dconv_down2 = double_conv(16, 32)\n",
    "        self.dconv_down3 = double_conv(32, 64)\n",
    "        self.dconv_down4 = double_conv(64, 128)\n",
    "        self.dconv_down5 = double_conv(128, 256)      \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1,1))       \n",
    "        self.fc = nn.Linear(256, 2) \n",
    "\n",
    "        self.maxpool = nn.MaxPool2d(2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv1 = self.dconv_down1(x)\n",
    "        x = self.maxpool(conv1)\n",
    "\n",
    "        conv2 = self.dconv_down2(x)\n",
    "        x = self.maxpool(conv2)\n",
    "        \n",
    "        conv3 = self.dconv_down3(x)\n",
    "        x = self.maxpool(conv3)   \n",
    "\n",
    "        conv4 = self.dconv_down4(x)\n",
    "        x = self.maxpool(conv4)\n",
    "\n",
    "        conv5 = self.dconv_down5(x)\n",
    "        x1 = self.maxpool(conv5)\n",
    "        \n",
    "        avgpool = self.avgpool(x1)\n",
    "        avgpool = avgpool.view(avgpool.size(0), -1)\n",
    "        outC = self.fc(avgpool)\n",
    "        \n",
    "        return conv5, conv4, conv3, conv2, conv1, outC\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, n_class = 1, nonlocal_mode='concatenation', attention_dsample = (2,2)):\n",
    "        super().__init__()\n",
    "\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "\n",
    "        self.dconv_up4 = double_conv(256 + 128 + 2, 128)\n",
    "        self.dconv_up3 = double_conv(128 + 64, 64)\n",
    "        self.dconv_up2 = double_conv(64 + 32, 32)\n",
    "        self.dconv_up1 = double_conv(32 + 16, 16)\n",
    "        self.conv_last = nn.Conv2d(16, n_class, 1)\n",
    "\n",
    "        self.conv_last_saliency = nn.Conv2d(17, n_class, 1)\n",
    "        \n",
    "        \n",
    "    def forward(self, input, conv5, conv4, conv3, conv2, conv1, saliency):\n",
    "  \n",
    "        bridge = torch.cat([input, saliency], dim = 1)\n",
    "        bridge = nn.functional.interpolate(bridge, scale_factor=0.125, mode='bilinear', align_corners=True)\n",
    "\n",
    "        x = self.upsample(conv5)\n",
    "        \n",
    "        '''\n",
    "        print(input.shape)\n",
    "        print(x.shape)\n",
    "        print(bridge.shape)\n",
    "        print(conv5.shape)\n",
    "        print(conv4.shape)\n",
    "        print(conv3.shape)\n",
    "        print(conv2.shape)\n",
    "        print(conv1.shape)\n",
    "        print(saliency.shape)\n",
    "        '''\n",
    "        \n",
    "        x = torch.cat([x, conv4, bridge], dim=1)\n",
    "\n",
    "        x = self.dconv_up4(x)\n",
    "        x = self.upsample(x)        \n",
    "        x = torch.cat([x, conv3], dim=1)       \n",
    "\n",
    "        x = self.dconv_up3(x)\n",
    "        x = self.upsample(x)        \n",
    "        # pdb.set_trace()\n",
    "        x = torch.cat([x, conv2], dim=1)\n",
    "\n",
    "        x = self.dconv_up2(x)\n",
    "        x = self.upsample(x)        \n",
    "        x = torch.cat([x, conv1], dim=1) \n",
    "\n",
    "        x = self.dconv_up1(x)\n",
    "        \n",
    "        out = self.conv_last(x)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = CovidMix(1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "## variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "clientsoclist = [0]*users\n",
    "\n",
    "start_time = 0\n",
    "weight_count = 0\n",
    "\n",
    "global_weights = copy.deepcopy(model.state_dict())\n",
    "\n",
    "datasetsize = [0]*users\n",
    "weights_list = [0]*users\n",
    "\n",
    "lock = Lock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_sendsize_list = []\n",
    "total_receivesize_list = []\n",
    "\n",
    "client_sendsize_list = [[] for i in range(users)]\n",
    "client_receivesize_list = [[] for i in range(users)]\n",
    "\n",
    "train_sendsize_list = [] \n",
    "train_receivesize_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0.0.0\n"
     ]
    }
   ],
   "source": [
    "#host = socket.gethostbyname(socket.gethostname())\n",
    "host = socket.gethostbyname(\"\")\n",
    "print(host)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = socket.socket()\n",
    "s.bind((host, port))\n",
    "s.listen(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training, Total time taken: 41m 53s\n"
     ]
    }
   ],
   "source": [
    "run_thread(receive, users)\n",
    "\n",
    "checkpoint_path = 'users_'+str(users)+'_rounds'+str(rounds)+'_model_epoch_'+str(local_epoch)+'.pth'\n",
    "torch.save(global_weights, checkpoint_path)\n",
    "\n",
    "print('Finished training, Total time taken: {:.0f}m {:.0f}s'.format((time.time() - start_time) // 60, (time.time() - start_time) % 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "name": "unet-resnet18.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
